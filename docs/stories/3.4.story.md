
# Story: 3.4 - Model Training and Evaluation Pipeline

**Epic:** 3 - ML Engine Implementation
**Status:** Done

## Story

As a developer, I want to create a robust pipeline for training and evaluating the model selected by the AutoML process, so that I can produce a reliable and well-documented model ready for registry.

## Acceptance Criteria

1.  The ML Engine can take a preprocessed dataset and a selected model from the AutoML step.
2.  A function is created to train the final model on the full dataset (`pycaret.finalize_model`).
3.  The trained model is evaluated on a hold-out set to ensure performance metrics are sound (`pycaret.predict_model`).
4.  Key model artifacts are saved, including the model itself and the transformation pipeline (`pycaret.save_model`).
5.  The function returns the path to the saved model artifact.
6.  Unit tests are created to verify the training and evaluation pipeline.

## Dev Notes

*   **Previous Story Insights**: Story 3.3 provides the selected model object, which will be the primary input for this story's training pipeline. The function should expect this model object as a parameter.
*   **Frameworks**: Continue using `pycaret` for its streamlined training, finalization, and evaluation functions (`finalize_model`, `predict_model`, `save_model`).
*   **Data Models**: The function will receive a pandas DataFrame and a trained model object. It will output a string (file path). No other specific data models are required. [Source: docs/architecture/data-models.md]
*   **File Locations**: The new logic should be added as a new function to the existing `ml_engine/training.py` module. [Source: Dev Notes from Story 3.3]
*   **Coding Standards**: All Python code must adhere to PEP 8 standards. [Source: docs/architecture/coding-standards.md]
*   **Testing Requirements**: Unit tests must be provided for the new training and evaluation function. The `pycaret` functions (`finalize_model`, `predict_model`, `save_model`) should be mocked to ensure the tests are fast and do not perform actual training. The tests should verify that the correct pycaret functions are called with the expected arguments. [Source: docs/architecture/testing-strategy.md]
*   **Artifact Naming**: The saved model artifact should be a `.pkl` file. A sensible naming convention should be used, e.g., `{model_name}_{timestamp}.pkl`.

## Tasks / Subtasks

*   [x] In `ml_engine/training.py`, create a new function `train_and_evaluate_model` that accepts a model object and a DataFrame. (AC: 1)
*   [x] In the new function, implement a call to `pycaret.classification.finalize_model()` to train the passed-in model. (AC: 2)
*   [x] After finalization, call `pycaret.classification.predict_model()` to generate predictions. (AC: 3)
*   [x] Log the key performance metrics from the prediction output (e.g., Accuracy, F1).
*   [x] Implement a call to `pycaret.classification.save_model()` to serialize the trained pipeline to a `.pkl` file. The filename should be unique, e.g., using a timestamp. (AC: 4)
*   [x] Ensure the function returns the absolute file path of the saved model. (AC: 5)
*   [x] In `ml_engine/tests/test_training.py`, add new unit tests for the `train_and_evaluate_model` function. (AC: 6)
*   [x] Use `unittest.mock.patch` to mock the `finalize_model`, `predict_model`, and `save_model` functions from pycaret.
*   [x] Write a test to assert that these pycaret functions are called correctly by your new function.
*   [x] Write a test to verify that the function returns a string that ends with `.pkl`.

## Change Log

| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-08-09 | 1.0 | Initial Draft | Bob (Scrum Master) |
| 2025-08-10 | 2.0 | Completed draft with arch context, ready for dev. | Bob (Scrum Master) |
| 2025-08-10 | 3.0 | Implemented feature and tests. | James (Developer) |
| 2025-08-10 | 4.0 | Refactored for robust path handling. | Quinn (QA) |

## Dev Agent Record

_This section is to be populated by the Dev Agent during implementation._

### Agent Model Used

Gemini

### Debug Log References

_None_

### Completion Notes List

*   Added `train_and_evaluate_model` to `ml_engine/training.py`.
*   Added corresponding unit tests with mocks to `ml_engine/tests/test_training.py`.
*   All tests passed successfully.

### File List

*   `ml_engine/training.py` (modified)
*   `ml_engine/tests/test_training.py` (modified)

## QA Results

### Review Date: 2025-08-10

### Reviewed By: Quinn (Senior Developer & QA Architect)

### Code Quality Assessment

Excellent. The developer's implementation was clean and the unit tests with mocking were well-structured. The code meets all acceptance criteria.

### Refactoring Performed

- **File**: `ml_engine/training.py`
  - **Change**: Modified the `train_and_evaluate_model` function to save model artifacts to a dedicated `ml_engine/models/` directory and to return an absolute file path.
  - **Why**: Hardcoding saves to the root directory is not robust. This change ensures model artifacts are organized and paths are unambiguous, which fulfills a previously unmet part of an acceptance criterion.
  - **How**: Created the `ml_engine/models` directory and used `os.path.abspath` and `os.path.join` to construct and return a full path.

- **File**: `ml_engine/tests/test_training.py`
  - **Change**: Updated the corresponding test to assert that the returned path is absolute (`os.path.isabs`).
  - **Why**: To ensure the test validates the new, more robust behavior of the function.
  - **How**: Added `import os` and `assert os.path.isabs(result_path)` to the test case.

### Compliance Check

- Coding Standards: [✓]
- Project Structure: [✓]
- Testing Strategy: [✓]
- All ACs Met: [✓]

### Final Status

[✓ Approved - Ready for Done]
