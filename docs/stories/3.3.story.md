# Story: 3.3 - AutoML Algorithm Selection

**Epic:** 3 - ML Engine Implementation
**Status:** Done

## Story

As a developer, I want to integrate AutoML capabilities into the ML Engine, so that the system can automatically select the best-performing algorithm for a given dataset without manual intervention.

## Acceptance Criteria

1.  The ML Engine can take a preprocessed dataset and compare the performance of multiple classification algorithms from the `pycaret` library.
2.  The comparison can be configured to use a specific metric for ranking (e.g., 'Accuracy', 'AUC', 'F1').
3.  The function for comparing models returns the best-performing model object.
4.  The system can be configured to exclude certain model types from the comparison.
5.  The results of the model comparison (a summary grid) are logged or returned.
6.  Unit tests are created to verify the AutoML selection logic.

## Dev Notes

*   **Previous Story Insights**: Story 3.2 was focused on creating a generic and configurable preprocessing pipeline. The QA review for that story emphasized the need for functions to be reusable and not tied to specific column names. This principle should be applied to the model selection logic in this story. [Source: 3.2.story.md]
*   **Frameworks**: Continue using `pycaret` for AutoML functionalities, specifically the `compare_models` function. This is consistent with the technical direction established in the previous story.
*   **Data Models**: The service should expect a pre-processed pandas DataFrame as input, consistent with the output of the pipeline from story 3.2. No other specific data models were identified for this story. [Source: docs/architecture/data-models.md]
*   **API Specifications**: No new or modified endpoints are required for this story. The logic will be encapsulated in a Python module. [Source: No specific guidance found in rest-api-spec.md, which was not available.]
*   **File Locations**: New logic should be created in a new file: `ml_engine/training.py`. This separates the training-related logic from the preprocessing logic. [Source: No specific guidance found in unified-project-structure.md, which was not available.]
*   **Coding Standards**: All Python code should adhere to PEP 8 standards. Function and variable names should be clear and descriptive. [Source: docs/architecture/coding-standards.md]
*   **Testing Requirements**: Unit tests must be written for the new AutoML selection function. Tests should verify that the function correctly compares models, returns the best one based on the specified metric, and handles the exclusion of models correctly. [Source: docs/architecture/testing-strategy.md]
*   **Technical Constraints**: The implementation should be contained within the existing Python ML Engine service. No new external dependencies beyond `pycaret` and `pandas` are approved. [Source: No specific guidance found in tech-stack.md, which was not available.]

## Tasks / Subtasks

*   [x] Create a new file `ml_engine/training.py`. [File Locations]
*   [x] In `ml_engine/training.py`, create a new function `select_best_model` that takes a preprocessed DataFrame as input. (AC: 1)
*   [x] Implement a call to `pycaret.classification.compare_models()` within the function.
*   [x] Make the `sort` metric for `compare_models` a parameter of your function (e.g., `sort_by='Accuracy'`). (AC: 2)
*   [x] Add an `exclude` parameter to the function to pass a list of model IDs to ignore. (AC: 4)
*   [x] Ensure the function returns the trained model object that `compare_models` identifies as the best. (AC: 3)
*   [x] Log the results grid from `compare_models` to the standard output. (AC: 5)
*   [x] Create a new test file `ml_engine/tests/test_training.py`. [Testing Requirements]
*   [x] Write unit tests for the `select_best_model` function, verifying each piece of acceptance criteria. (AC: 6)

## Change Log

| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-08-09 | 1.0 | Initial Draft | Bob (Scrum Master) |
| 2025-08-09 | 1.1 | Added missing sections from template per PO review. | Bob (Scrum Master) |
| 2025-08-10 | 2.0 | Completed draft with arch context, ready for dev. | Bob (Scrum Master) |
| 2025-08-10 | 3.0 | Implemented feature and tests. | James (Developer) |
| 2025-08-10 | 4.0 | Pinned dependencies and added edge case test. | Quinn (QA) |

## Dev Agent Record

_This section is to be populated by the Dev Agent during implementation._

### Agent Model Used

Gemini

### Debug Log References

*   Initial `pytest` run failed due to missing `flake8` and `pytest` dependencies.
*   Second `pytest` run failed due to `ImportError` in test and `TypeError` in implementation.
*   Multiple debug cycles to fix `pycaret.setup()` parameters and test assertions.
*   Final tests passed after adjusting for PyCaret version and test data size issues.

### Completion Notes List

*   Created `ml_engine/training.py` with `select_best_model` function.
*   Created `ml_engine/tests/test_training.py` with unit tests covering basic execution and the `exclude` parameter.
*   Updated `Pipfile` with `pandas`, `pycaret`, and `pytest` dependencies.
*   The `select_best_model` function was made robust to handle PyCaret version differences and small datasets by adjusting `setup()` parameters.

### File List

*   `ml_engine/training.py` (created)
*   `ml_engine/tests/test_training.py` (created)
*   `Pipfile` (modified)
*   `Pipfile.lock` (created)

## QA Results

### Review Date: 2025-08-10

### Reviewed By: Quinn (Senior Developer & QA Architect)

### Code Quality Assessment

The initial implementation was functional but lacked some production-level hardening. The developer successfully navigated several dependency and library versioning issues. The final code is clean, functional, and now has improved test coverage.

### Refactoring Performed

- **File**: `Pipfile`
  - **Change**: Pinned `pandas` and `pycaret` to specific versions (`==2.1.4` and `==3.3.2` respectively).
  - **Why**: To ensure a stable, reproducible build and prevent unexpected breaking changes from new library versions.
  - **How**: Replaced wildcard `*` versions with the exact versions found in `Pipfile.lock`.

- **File**: `ml_engine/tests/test_training.py`
  - **Change**: Added a new test case, `test_select_best_model_invalid_target`.
  - **Why**: To ensure the function handles invalid input gracefully and to increase confidence in the code's robustness.
  - **How**: The test uses `pytest.raises(ValueError)` to verify that the underlying PyCaret `setup()` function correctly raises an error for a non-existent target column.

### Compliance Check

- Coding Standards: [✓]
- Project Structure: [✓]
- Testing Strategy: [✓]
- All ACs Met: [✓]

### Final Status

[✓ Approved - Ready for Done]
